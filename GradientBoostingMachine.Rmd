---
title: "Gradient Boosting Machine"
author: "Shivangi Vashi"
date: "6/25/2020"
output: html_document
---

<center>
$\large Association~Rules~Mining~using~Gradient~Boosting$ </center>


<br><br>


#### Loading data


```{r}

source_code_filepath<-"readandsampledata.R" 
cat("Loading file ", source_code_filepath, "\n")
source(file=source_code_filepath)

# aisles<-fread("instacart-market-basket-analysis/aisles.csv")
# departments<-fread("instacart-market-basket-analysis/departments.csv")
# order_products_prior<-fread("instacart-market-basket-analysis/order_products__prior.csv")
# order_products_train<-fread("instacart-market-basket-analysis/order_products__train.csv")
# orders<-fread("instacart-market-basket-analysis/orders.csv")
# products<-fread("instacart-market-basket-analysis/products.csv")


```
<br>

We recode variables as factors for later use in feature engineering.
```{r}

orders<-orders %>%mutate(order_hour_of_day=as.numeric(order_hour_of_day))
orders$eval_set <-as.factor(orders$eval_set)
aisles$aisle <- as.factor(aisles$aisle)
departments$department <- as.factor(departments$department)
products$product_name <- as.factor(products$product_name)

```


#### Feature engineering
<br>
We perform feature engineering to create features for the xgboost model. The Xgboost is a package that allows you to create gradient boosting model- a number of weak learning decision trees that improve with each iteration.<br>

The following are the features we created:<br>
-   Product reorder probability<br>
-   Average Cart Position<br>
-   User order probability<br>
-   user orders, which is the total number of orders a user makes<br>
-   user product orders, ie how many products does a user order<br>
<br>


```{r}
# xgboost- Gradient Boosting
library(xgboost)
# Creating a product level by combing product, department, aisles
product_level <- merge(x = products, y = aisles, by = "aisle_id")
product_level <- merge(x = product_level, y = departments, by = "department_id")
product_level$department_id <- NULL
product_level$aisle_id <- NULL
product_level <- arrange(product_level, product_id)



# combining order data and prior data
ordered_products <- merge(x = orders, y = order_products_prior, by = "order_id")



# product reorder probability and avg_cart_position of each cart
product_prob<-ordered_products %>%
        arrange(user_id, order_number, product_id) %>%
        group_by(product_id) %>% 
        summarise(product_orders = n(), product_reorders = sum(reordered), avg_cart_pos = mean(add_to_cart_order))

product_prob$reorder_prob <- product_prob$product_reorders/product_prob$product_orders
product_prob$product_reorders <- NULL

# calculating user buy prob

# calculating user order probability by looking at prior orders 
users_prob<-orders %>%
          filter(eval_set == "prior") %>%
          group_by(user_id) %>% 
          summarise(user_orders = max(order_number), user_period = sum(days_since_prior_order, na.rm = TRUE), avg_days_since_prior = mean(days_since_prior_order, na.rm = TRUE))

# calculating total_products, reorder probability and num of products
users_reorder_prob <- ordered_products %>%
              group_by(user_id) %>%
              summarise(user_total_products = n(), user_reorder_prob = sum(reordered == 1) / sum(order_number > 1),num_products = n_distinct(product_id))

# merging above two to get user level
users_prob<- merge(x = users_prob, y = users_reorder_prob, by = "user_id", all.x = TRUE)

```



##### Dividing the data into train and test
<br>
The orders table has a flag eval_set that indicates whether the data is for training or testing. So we divide the data using this flag into train and test. The test data has no information about whether the product was reordered or not, so we use this to test our data on. 
<br>
```{r}

# filtering training and testing data from orders
train_test <- orders %>% filter(eval_set != "prior") %>% select(user_id, order_id, eval_set, days_since_prior_order)

# left join users_prob with train_test data
users_prob <- merge(x = users_prob, y = train_test, all.x = TRUE)




# calculating average cart position and total orders of each product user purchased
user_product_cart <- ordered_products %>% group_by(user_id, product_id) %>% 
  summarise( user_product_orders = n(), avg_user_product_pos = mean(add_to_cart_order))



# now combining all the user_level and product _level info
user_product_cart <- merge(user_product_cart,product_prob, by = "product_id", all.x = TRUE)
user_product_cart <- merge(user_product_cart,users_prob, by = "user_id", all.x = TRUE)


#taking only user_ids that are common in orders
order_products_train$user_id <- orders$user_id[match(order_products_train$order_id, orders$order_id)]
# combining training data and infered data by product id and user id
order_products_train <- order_products_train %>% select(user_id, product_id, reordered)
user_product_cart <- merge(user_product_cart, order_products_train, by = c("user_id", "product_id"), all.x = TRUE)



# training data
train <- as.data.frame(user_product_cart[user_product_cart$eval_set == "train",])
# removing char - xgboost
train$eval_set <- NULL
#no need
train$user_id <- NULL
train$product_id <- NULL
train$order_id <- NULL
train$reordered[is.na(train$reordered)] <- 0

# testing data
test <- as.data.frame(user_product_cart[user_product_cart$eval_set == "test",])
test$eval_set <- NULL
test$user_id <- NULL
test$reordered <- NULL



#we got our training and testing data
# we have taken only numeric data because we will be using xgboost



# Parameters for the xgboost model
params <- list(
  # logistic model
  "objective"           = "reg:logistic",
  
  # logless for cross-validation
  "eval_metric"         = "logloss", 
  
  #learning rate
  "eta"                 = 0.1,
  
  #depth of tree
  "max_depth"           = 6, 
  
  # min sum of weights
  # should be high enough to prevent over fitting
  # but not too high for over fitting
  "min_child_weight"    = 10,
  
  # the min loss value require to split
  "gamma"               = 0.70,
  
  # fraction of observations to be included in each tree 
  # generally varies from 0.5-1
  "subsample"           = 0.75,
  
  # fraction of column to be randomly sample in each tree
  "colsample_bytree"    = 0.95,
  
  # regularization coefficients
  "alpha"               = 2e-05,
  "lambda"              = 10 
)

```


#### XGBoost Model

We create the xgboost model using xgboost(), iterated 80 times. The data is restructured a xgb.DMatrix and the parameters are given as a list.<br>

We test our model and get an error of 0.1185, which is very low.<br>

Finally, this model can be used for predicting product reorders or associations, which are found by:<br>
-   filtering by only those products that were reordered<br>
-   grouping them by order id <br>
-   summarising all products associated with that order id. <br>
<br>
```{r}


X <- xgb.DMatrix(as.matrix(train %>% select(-reordered)), label = train$reordered)
model <- xgboost(data = X, params = params, nrounds = 80)

importance <- xgb.importance(colnames(X), model = model)
xgb.ggplot.importance(importance)

X2 <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))
# predicting reordered values from test dataset
test$reordered <- predict(model, X2)



#Test error 
p<-test$reordered
err <- mean(as.numeric(p > 0.5) != train$reordered)
print(paste("test-error=", err))

```

<br><br>

As we can see from this plot, the user_product_orders and user_orders features are the most important in predicting whether a product will be reordered or not.

<br>
<br>
#### Generating Association Rules using Gradient Boosting Model

We finally use this gradient boosting model to generate association rules within the test dataset. 
```{r}

test$reordered <- (test$reordered > 0.21) * 1


      
# summarise as order_id and products
rules <- test %>% filter(reordered == 1) %>% group_by(order_id) %>%
        summarise(products = paste(product_id, collapse = " "))

# filling the missing values
missing <- data.frame(
  order_id = unique(test$order_id[!test$order_id %in% rules$order_id]), products = "None")
rules <- rules %>% bind_rows(missing) %>% arrange(order_id)

head(rules,20)


rulesgbm <- test%>%inner_join(products)%>%
            filter(reordered == 1) %>% group_by(order_id) %>%
            summarise(rules = paste(product_name, collapse = ","))

head(rulesgbm,20)

```

<center>
$\large Conclusion$ </center>

<br><br>

-   We explored the data to analyze buying patterns and popular products<br>

-   We performed logistic regression to predict whether a product will be reordered, with 0.53% accuracy. <br>

-   We implemented apriori algorithm for associations rule mining to generate strong rules that dictate what products are most likely to be bought together<br>

-   We use the gradient boosting model to create a highly accurate model that predicts whether a product will be reordered, and we use this model to further generate association rules in the test data  for associations rule mining to generate strong rules that dictate what products are most likely to be bought together<br>

-   We use the gradient boosting model to create a highly accurate model that predicts whether a product will be reordered, and we use this model to further generate association rules in the test data<br>



