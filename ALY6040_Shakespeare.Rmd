---
title: "ALY6040_Shakespeare"
author: "Shivangi Vashi"
date: "6/17/2020"
output: html_document
---


<center>


### ALY 6040 Data Mining Applications
### Assignment 2: Instacart Linear and Logistic Regression Model
### Shivangi Vashi
### Yihong Qiu
### Md Tajrianul Islam

<br> <br> <br> <br>
<br> <br> <br> <br>
  
### Instructor: Kasun Samarasinghe
### Spring 2020
### June 17 2020
### Northeastern University
</center>

<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>

<style>
body {
text-align: justify}
</style>

<center>
$\LARGE Introduction$  </center>
<br><br>
Today we are going to perform text mining. It is the process and analyzing text data- which is unstructured and find underlying patterns. Here are the following steps we will follow:

-Text organizations

-Feature extraction

-Analysis

We are using the works of Shakespeare for this analysis.

Bag of words: treats words as singular tokens and analyzes them separately.



##### Loading data

We divide the text file into 3 to better view the corpus and word count matrix. A corpus is a collection of documents- it is a datatype you can create using the 'tm' package.
```{r warning=FALSE, message=FALSE}

library(RCurl)
library(tm)
library(SnowballC)
library(wordcloud)

dir.create("data")
setwd("data")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/1.txt"
write(getURL(url), file = "1.txt")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/2.txt"
write(getURL(url), file = "2.txt")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/3.txt"
write(getURL(url), file = "3.txt")
setwd("..")

```
<br><br>


<center>
$\LARGE Analysis$  </center>
<br><br>

Next, we make a corpus object for our documents. There are 2 kinds- Pcorpus (permanent) and Vcorpus(volatile). We will use Vcorpus that will be stored in the RAM, to be more memory efficient.

R needs to interpret each element in our text file as a document. The Source functions are used for this purpose. We use a Source function called DirSource() because our text data is contained in a directory. The output of this function is called a Source object.
<br>
```{r}

#we need to provide a source and there are three 
#types of such sources, DirSource, VectorSource and 
#DataFrameSource. We will use DirSource the import the 
#three text files that we just downloaded and using 
#DirSource is the only way to import files from the user system


shakespeare <- VCorpus(DirSource("data", encoding = "UTF-8"))
shakespeare

#Corpus mainipulation
#There are many methods we can use to play with Corpus. 
#Each Corpus has its own meta data and each document in the 
#Corpus also has one.

meta(shakespeare[[1]])

shakespeare[[1]]
summary(shakespeare)

#There are also other useful methods available such as 
#tmUpdate() which checks the new files that do not exist 
#yet and add those in and inspect() which gives a more 
#detailed overview than summary().

```
<br><br>

#### Data Cleaning

It is important to clean up the data before proceeding to analyse it, because there may be words and aspects of the data we do not care about. So we first remove the white space from the data. We then perform stemming to reduce all common words to its root word. We must therefore use stemDocument(). tm_map is a fast and effective way to apply the operation to all of the documents. Without it we would have to split and unlist the documents so that the whole document is not treated as one character vector.

The argument lazy=TRUE is there so that the mapping is delayed until all of the content is accessed.
We also remove punctiation and remove stopwords such as a an the, etc. First we take a look at all the stopwords ie highly frequent words in english that don't provide any information about the text using stopwords("en").

There is a lot of information in those documents which is not particularly useful for text mining. So before proceeding any further, we will clean things up a bit. First we strip extra whitespace from a text document,followed by stemming words which removes affixes from words (so, for example, “run”, “runs” and “running” all become “run”) and then remove punctuation, numbers and common English stopwords. Possibly the list of English stop words is not entirely appropriate for Shakespearean English, but it is a reasonable starting point.

<br><br>
```{r}


# Remove whitespace
shakespeare <- tm_map(shakespeare, stripWhitespace, lazy=TRUE)
# Stemming 
shakespeare <- tm_map(shakespeare, stemDocument, lazy=TRUE)
# Remove punctuation
shakespeare <- tm_map(shakespeare, removePunctuation, lazy=TRUE)


#look at common english stopwords
stopwords("en")

# Remove stopwords
shakespeare<-tm_map(shakespeare, content_transformer(removeWords),stopwords("en"))

# Case conversion
shakespeare<-tm_map(shakespeare, content_transformer(tolower), lazy = TRUE)
```

<br><br>
If we have a look at what’s left, we find that it’s just the lowercase, stripped down version of the text


#### Preprocessing and Analysis
A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis. Next we create a Term Document Matrix (TDM) which reflects the number of times each word in the corpus is found in each of the documents.

```{r}
#Term document matrix
#at this point our data is ready for analysis

dtm <- DocumentTermMatrix(shakespeare)
```

Then we look at the terms which has appeared most frequently. 

```{r}
#Essentially the frequency of a word is a representation 
#of its importance. We have see the terms that show 
#up more than 25 times

highFreqTerms <- findFreqTerms(dtm, 25, Inf)
summary(highFreqTerms)

highFreqTerms[1:10]
```

There are also associations between words. Let’s have a look at what other words had a high association with “love”.
```{r}
#We can also find the words that have a 
#certain correlations with one word in the 
#term document matrix. The correlation value is one 
#if two words always appear together and it becomes 
#zero if they never show up at the same time. 
#We first want to see how many words have a correlation 
#higher than 0.95 with the word love.

loves_assocs <- findAssocs(dtm, "love", 0.95)
```
<br><br>
The colour scale indicates the number of times that each of the terms cropped up in each of the documents.
<br>
```{r}
#visualization on all the words that have frequency higher than 2500.


freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)


set.seed(555)
wordcloud(names(freq), freq, min.freq=2500, max.words = 100, colors=brewer.pal(8, "Dark2"))


barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=heat.colors(50))


```


<center>
$\LARGE References$  </center>
<br><br>
https://campus.datacamp.com/courses/text-mining-with-bag-of-words-in-r/jumping-into-text-mining-with-bag-of-words?ex=6
