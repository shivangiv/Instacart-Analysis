---
title: "ALY6040_Shakespeare"
author: "Shivangi Vashi"
date: "6/17/2020"
output: html_document
---


<center>


### ALY 6040 Data Mining Applications
### Assignment 2: Instacart Linear and Logistic Regression Model
### Shivangi Vashi
### Yihong Qiu
### Md Tajrianul Islam

<br> <br> <br> <br>
<br> <br> <br> <br>
  
### Instructor: Kasun Samarasinghe
### Spring 2020
### June 17 2020
### Northeastern University
</center>

<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>

<style>
body {
text-align: justify}
</style>

<center>
$\LARGE Introduction$  </center>
<br><br>
Today we are going to perform text mining. It is the process and analyzing text data- which is unstructured and find underlying patterns. Here are the following steps we will follow:

-Text organizations

-Feature extraction

-Analysis

We are using the works of Shakespeare for this analysis.

Bag of words: treats words as singular tokens and analyzes them separately.



##### Loading data

We divide the text file into 3 to better view the corpus and word count matrix. A corpus is a collection of documents- it is a datatype you can create using the 'tm' package.
```{r warning=FALSE, message=FALSE}
#In order to have a better view of Corpus and word count matrix, 
#we divide the original text file into three and you can download 
#them from 1.txt, 2.txt and 3.txt.

#Alternatively you can just run the following commands the 
#download and store the files in a folder called data:

library(RCurl)
library(tm)
library(SnowballC)
dir.create("data")
setwd("data")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/1.txt"
write(getURL(url), file = "1.txt")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/2.txt"
write(getURL(url), file = "2.txt")
url <- "https://raw.githubusercontent.com/angerhang/statsTutorial/master/src/textMining/data/3.txt"
write(getURL(url), file = "3.txt")
setwd("..")

```


Next, we make a corpus object for our documents. There are 2 kinds- Pcorpus (permanent) and Vcorpus(volatile). We will use Vcorpus that will be stored in the RAM, to be more memory efficient.

R needs to interpret each element in our text file as a document. The Source functions are used for this purpose. We use a Source function called DirSource() because our text data is contained in a directory. The output of this function is called a Source object.

```{r}

#we need to provide a source and there are three 
#types of such sources, DirSource, VectorSource and 
#DataFrameSource. We will use DirSource the import the 
#three text files that we just downloaded and using 
#DirSource is the only way to import files from the user system


shakespeare <- VCorpus(DirSource("data", encoding = "UTF-8"))
shakespeare

#Corpus mainipulation
#There are many methods we can use to play with Corpus. 
#Each Corpus has its own meta data and each document in the 
#Corpus also has one.

meta(shakespeare[[1]])

shakespeare[[1]]
summary(shakespeare)

#There are also other useful methods available such as 
#tmUpdate() which checks the new files that do not exist 
#yet and add those in and inspect() which gives a more 
#detailed overview than summary().

```


#### Data Cleaning

It is important to clean up the data before proceeding to analyse it, because there may be words and aspects of the data we do not care about. So we first remove the white space from the data. We then perform stemming to reduce all common words to its root word. We must therefore use stemDocument(). tm_map is a fast and effective way to apply the operation to all of the documents. Without it we would have to split and unlist the documents so that the whole document is not treated as one character vector.

The argument lazy=TRUE is there so that the mapping is delayed until all of the content is accessed.
We also remove punctiation and remove stopwords such as a an the, etc. First we take a look at all the stopwords ie highly frequent words in english that don't provide any information about the text using stopwords("en").

```{r}
library(wordcloud)
#Cleaning data
#We now want to transform the data we have before we actually conduct out analysis. Some classical method for cleaning are removing unnecessary white space, stemming and stop words removal and conversion to lower or upper case. For the ones don't know, stop words removal is to eliminate some common words that have little value in the analysis e.g. the and be.
#Stem mining is to reduce derived words to their root form such as making says, said into say because they essentially have the same meanings. We do the cleaning with the help of tm_map which applies a function across all the documents.

# Remove whitespace
shakespeare <- tm_map(shakespeare, stripWhitespace, lazy=TRUE)
# Stemming 
shakespeare <- tm_map(shakespeare, stemDocument, lazy=TRUE)
# Remove punctuation
shakespeare <- tm_map(shakespeare, removePunctuation, lazy=TRUE)

# List standard English stop words
stopwords("en")
shakespearestops<-c("a","an","the")
# Remove stopwords
shakespeare<-tm_map(shakespeare, content_transformer(removeWords), stopwords(shakespearestops), lazy=TRUE)

# Case conversion
shakespeare<-tm_map(shakespeare, content_transformer(tolower), lazy = TRUE)



#Term document matrix
#at this point our data is ready for analysis

dtm <- DocumentTermMatrix(shakespeare)

#Essentially the frequency of a word is a representation 
#of its importance. We have see the terms that show 
#up more than 25 times

highFreqTerms <- findFreqTerms(dtm, 25, Inf)
summary(highFreqTerms)

highFreqTerms[1:10]

#We can also find the words that have a 
#certain correlations with one word in the 
#term document matrix. The correlation value is one 
#if two words always appear together and it becomes 
#zero if they never show up at the same time. 
#We first want to see how many words have a correlation 
#higher than 0.95 with the word love.

loves_assocs <- findAssocs(dtm, "love", 0.95)

#visualization on all the words that have frequency higher than 2500.


freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)


set.seed(555)
wordcloud(names(freq), freq, min.freq=2500, max.words = 100, colors=brewer.pal(8, "Dark2"))


barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=heat.colors(50))




```

References:
https://campus.datacamp.com/courses/text-mining-with-bag-of-words-in-r/jumping-into-text-mining-with-bag-of-words?ex=6