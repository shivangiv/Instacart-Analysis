---
title: "InstacartFinalProject"
author: "Shivangi Vashi"
date: "6/23/2020"
output: html_document
number_sections: TRUE

---
<center>


### ALY 6040 Data Mining Applications
### Assignment 3: Instacart Final Project
### Shivangi Vashi
### Yihong Qiu
### Md Tajrianul Islam

<br> <br> <br> <br>
<br> <br> <br> <br>
  
### Instructor: Kasun Samarasinghe
### Spring 2020
### June 23 2020
### Northeastern University
</center>

<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>

<style>
body {
text-align: justify}
</style>

<center>
$\LARGE Introduction$  </center>
<br><br>

Instacart is a grocery delivery application that works in the following way- users can select products through the app, and then personal shoppers review the order, shop in-store and deliver the products to the users.
We got the data from Instacart's Market Basket Analysis on kaggle.com [here](https://www.kaggle.com/c/instacart-market-basket-analysis/overview).

The Instacart data set is anonymized and contains samples of over 3 million grocery orders from 200,000+ Instacart users.  <br>

The reason we chose this data is because it is varied and some very clear applications for resolving business questions. It will allow us to perform different data mining algorithms and analyze it in different ways. It also has time information with enough granularity, so the option of some time series analysis is also there. <br><br>

In this final project, there are four parts of the Instacart Market Basket analysis. The first part is EDA.
The second part is Association Rule Mining. The third part is Logistic Regression. The fourth part is model selection for Logistic Regression via Association Rule Mining. 
<br><br>


Reading the dataset,importing relevant libraries.

```{r global_options, include=FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r message=FALSE,warning=FALSE}
library(plyr)
library(data.table)
library(tidyverse)
library(arules)
library(arulesViz)
library(plotly)
library(IRdisplay)
library(grid)
library(caret)
library(glmnet)


#using fread because it reads data very fast
aisles<-fread("instacart-market-basket-analysis/aisles.csv")
departments<-fread("instacart-market-basket-analysis/departments.csv")
order_products_prior<-fread("instacart-market-basket-analysis/order_products__prior.csv")
order_products_train<-fread("instacart-market-basket-analysis/order_products__train.csv")
orders<-fread("instacart-market-basket-analysis/orders.csv")
products<-fread("instacart-market-basket-analysis/products.csv")

```
<br><br>


<center>
$\large Data~Wrangling$ </center>
<br><br>

#### Data Preparation

```{r message=FALSE}
# Taking a quick look at the data
head(aisles)
head(departments)
head(order_products_prior)
head(order_products_train)
head(orders)
head(products)

#Remove missing values
aisles<-drop_na(aisles)
departments<-drop_na(departments)
order_products_prior<-drop_na(order_products_prior)
order_products_train<-drop_na(order_products_train)
orders<-drop_na(orders)
products<-drop_na(products)


# Check if all classes ie data types is correct
sapply(aisles,class)
sapply(departments,class)
sapply(order_products_prior,class)
sapply(order_products_train,class)
sapply(orders,class)
orders<-orders %>%mutate(order_hour_of_day=as.numeric(order_hour_of_day))
orders
sapply(products,class)

#Data Preparation
#We recode and convert character variables to factors.
orders$eval_set <-as.factor(orders$eval_set)
aisles$aisle <- as.factor(aisles$aisle)
departments$department <- as.factor(departments$department)
products$product_name <- as.factor(products$product_name)
```
<br>


Since the dataset is very large, with Prior orders having 32 million rows, we subset the data to reduce calculation time. We did this by randomly sampling users, then only keeping their orders and prior order information by performing inner joins with the order prior and train datasets.

<br>

```{r}
set.seed(123)
user_fraction <- 0.1
users <- unique(orders$user_id)
sample_users <- sample(users, round(user_fraction * length(users)))

cat('Number of orders (before): ', nrow(orders))
orders <- orders[user_id %in% sample_users]
cat('Number of orders (after): ', nrow(orders))

order_products_prior<-order_products_prior%>%semi_join(orders)
order_products_train<-order_products_train%>%semi_join(orders)


# Training dataset
OrderProductPrior<-orders%>%inner_join(order_products_prior)
OrderProductPrior<-drop_na(OrderProductPrior)


#Testing dataset
OrderProductTrain<-orders%>%inner_join(order_products_train)
OrderProductTrain<-drop_na(OrderProductTrain)


dim(OrderProductPrior)
dim(OrderProductTrain)
head(OrderProductPrior)
head(OrderProductTrain)

```
<br><br>

<center>
${\large Data Exploration}$ </center>
<br><br>

#### Ordering patterns 
<br>
First, we find out most orders are placed during 9am to 5 pm during the day. Additionally, assuming Sunday is the first, ie 0 = Sunday in the dataset, Sundays and Mondays are when most orders are placed.
<br>

```{r warning=FALSE}
# When do people order most during the day?
orders%>%
  group_by(order_hour_of_day)%>%
  summarise(Number_of_Orders=n())%>%
  ggplot(aes(y=Number_of_Orders, x=order_hour_of_day, fill=Number_of_Orders)) + geom_col()+ coord_cartesian(xlim = c(0, 24))+ labs(y="Number of Orders", x="Hour of the Day")

#What days of the week do people order during the week
orders%>%
  group_by(order_dow)%>%
  summarise(Number_of_Orders=n())%>%
  ggplot(aes(y=Number_of_Orders, x=order_dow, fill=Number_of_Orders)) + geom_col()+ labs(y="Number of Orders", x="Day of the Week starting Sunday" )


```
<br><br>

Clearly, most orders are placed during 9am to 5 pm during the day. Additionally, assuming Sunday is the first, ie 0 = Sunday in the dataset, Sundays and Mondays are when most orders are placed.

Using this information, Instacart could redirect their resources so that the higher volume of orders can be processed at these times efficiently. <br>
<br> <br>

#### 2. What is the frequency of reordering?
<br>
When exploring the frequency of reordering, first we filter prior eval_set and select data of days_since_prior_order, then we also clease all the na value from the dataset. 
We find out in this 3,000,000 orders+ dataset, the average days of users who reorder their groceries is from 7 to 11 days. Many customers reorder after 7 days or 30 days.
According to each customer's reordering behavior and how many days they would reorder through instacart,we can predict their next ordering day. Therefore, based on these analysis, it can provide suggestions to retails and suppliers which would be helpful for their purchase sales inventory stategies.

In the future, we still need to find out the reason why a lot customers reorder after 30 days and what products do they get then we can draw the conculsion. Also, we will find out the correlation among the frequency of reordering, order hour of day, and ordering products and tracking customer consuming behaviors.


<br>
```{r}
summary(orders$days_since_prior_order)

ggplot(orders, aes(x=days_since_prior_order))+ 
  geom_boxplot(mapping=aes("var",days_since_prior_order),colour="black",fill="#AC88FF")+
  xlab("")+
  ylab("days_since_prior_order")+
  ggtitle("Days since Prior Order Boxplot")

ggplot(orders, aes(x=days_since_prior_order))+ 
  geom_histogram(aes(y =..density..),stat = "bin", bins= 30, colour="black", fill="lavenderblush")+
  geom_density(alpha=.2,fill="plum")+
  ggtitle("Days since Prior Order Density Plot")

```
<br><br>



#### 3. What are the most popular products?
<br><br>
By descending ordering products that are added into cart order, we find out the top 10 popular products out of 49,688 products as shown below. Surprisely, in our analysis, the top No.1 popular product is Organic 100% Grapefruit Juice.

It is important to find out what products people like to order and order the most, by doing so, we can have better prediction and preparation on the stock of these products. 
In our next steps, we will find out which departmants and aisles these products belong to, and how often do these top products reorder?

<br><br>
```{r}
inner_join(order_products_prior, products, by =c("product_id"))%>%
        as.data.frame()%>%
        select(5, 3)%>%
        head(arrange(desc(pop_products$add_to_cart_order)), n = 10)%>%
        ggplot()+
        geom_bar(aes(reorder(product_name,-add_to_cart_order),add_to_cart_order, fill= add_to_cart_order),
        stat = "identity", color = "grey", fill = "#00BFC4") +
        labs(title = "Most Popular Products Added to Cart Order", x = "product names", y = "add to cart order") +
        theme(axis.text.x = element_text(angle = 65, hjust = 1))

```
<br><br>

To recommend the next item an user is most likely to add to his/ her cart, we have to understand how each group of users use the service. It becomes a classification problem that we can explore later, but for now we wanted to see how different users purchase products, how there can be correlation between their total number of orders, days since prior order, no. of items they add to their cart and their probability of reordering.
<br><br>

### 4. Which Users Reorder?

To recommend the next item an user is most likely to add to his/ her cart, we have to understand how each group of users use the service. It becomes a classification problem that we can explore later, but for now we wanted to see how different users purchase products, how there can be correlation between their total number of orders, days since prior order, no. of items they add to their cart and their probability of reordering.   

```{r}
#user clusters
n_items_per_order <- OrderProductPrior %>% group_by(order_id) %>% mutate(n_items=max(add_to_cart_order))
user <- n_items_per_order %>% 
  group_by(user_id) %>% 
  mutate(product_name = as.numeric(days_since_prior_order)) %>%
  summarize(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = mean(n_items), avg_reorder=mean(reordered))
head(user)
```
<br>


It seems many of the most popular products are healthy vegetables and fruit. This could be direction for target advertising.
<br><br>


So what we can see from the table the people who have ordered more number of times, they have higher possibility of reordering the same products and they usually order after every three to four weeks. Which means that the products that gets ordered more will also have the higher probablity of being reordered.


```{r}
order_products_train %>% 
  group_by(product_id) %>% 
  summarize(proportion_reordered = mean(reordered), n=n()) %>%
  ggplot(aes(x=n,y=proportion_reordered))+
  geom_point()+
  geom_smooth(color="red")+
  coord_cartesian(xlim=c(0,2000))

```



#### 5. Most popular department and most popular aisle?

So we wanted to see which are the most popular departments and aisles, as aparently these are the same department and aisles where will most reorder must occur. Later we can also find if which other product has the more probablity of being purchased with other. We can find that using association rule mining. To see the most popular departments and aisles we create a treemap where the size of the boxes shows the number of sales.

```{r message=FALSE}
library(treemap)
tmp <- products %>%
        group_by(department_id, aisle_id) %>% summarize(n=n())%>%
        left_join(departments,by="department_id")%>%
        left_join(aisles,by="aisle_id")

order_products_train %>% 
  group_by(product_id) %>% 
  summarize(count=n()) %>% 
  left_join(products,by="product_id") %>% 
  ungroup() %>% 
  group_by(department_id,aisle_id) %>% 
  summarize(sumcount = sum(count)) %>% 
  left_join(tmp, by = c("department_id", "aisle_id")) %>% 
  mutate(onesize = 1)%>%
  treemap(product_portfolio,index=c("department","aisle"),vSize="sumcount",title="",palette="Set3",border.col="#FFFFFF")
```

<br><br>
What we can see is produce, diary eggs are the most popular departments followed by snacks, pantry and others. But talking about departments, organic and non organic sector divides US consumers in a large way these days. So it will be interesting to see which products get most reordered.

<br>


<center>
$\large Logistic Regression$ </center>
<br><br>

```{r}
## Preparing data for modelling
OrderProductPrior_log<-OrderProductPrior%>%inner_join(products)
OrderProductPrior_log<-drop_na(OrderProductPrior_log)
OrderProductPrior_log<-OrderProductPrior_log[-c(3)]

OrderHour <- OrderProductPrior_log[-c(1,2,7,10)]

OrderProductTrain_log<-orders%>%inner_join(order_products_train)%>%inner_join(products)
OrderProductTrain_log<-drop_na(OrderProductTrain_log)
OrderProductTrain_log<-OrderProductTrain_log[-c(3)]
```

<br>
We perform logistic regression to predict the variable 'reorder'. Since it is a binary variable with 2 classes,1 and 0, it becomes a classification problem.
As we know from our EDA, Instacart has a lot of loyal customers, who order bi-weekly or monthly. While ordering they also reorder a lot of the same products. So predicting whether a user will reorder or not, may help us later to understand what products to recommend while they are purchasing.
<br>

```{r}
user <- OrderProductPrior_log %>% 
  group_by(user_id) %>% 
  mutate(days_since_prior_order = as.numeric(days_since_prior_order)) %>%
  transmute(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = max(add_to_cart_order), reordered= reordered, order_day = order_dow, product_id=product_id)

reorder_log <- user[-c(1)]
head(reorder_log)
```
<br><br>

So we created a new dataframe dropped the user_id and use all the other columns to predict the reorders with logistic regression. Our regression depends variables such as the total number orders a user has, after how many days he is ordering, how many products he is adding to the cart, which day he is ordering and which products usually get reordered. For a example if a user usually shops bi weekly, on sunday and orders milk; he might have very less chances of ordering milk if he is back withing 2 or 3 days of his last purchase.

#### Model
We use glm with family="binomial" to create the model. We use all of the variables for this model.
```{r}
#Fitting a binary logistic regression
log_model <- glm(reordered ~., data = reorder_log, family = "binomial")
#Model summary
summary(log_model)

```


#### Predicting using the model

We predict the reorder variable using predict function.
```{r}
library(e1071)
user2 <- OrderProductTrain_log %>% 
  group_by(user_id) %>% 
  mutate(days_since_prior_order = as.numeric(days_since_prior_order)) %>%
  transmute(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = max(add_to_cart_order), reordered= reordered, order_day = order_dow, product_id=product_id)

reorder_log2 <- user2[-c(1)]
head(reorder_log2)

#Prediction
pred <- predict(log_model, reorder_log2, type = "response")
#If p > 0.5, then Class is 1 else 0
y_pred <- ifelse(pred > 0.5, 1, 0)
y_act <- reorder_log2$reordered

#Accuracy
mean(y_pred == y_act) 

## Another method
y_pred<-as.factor(y_pred)
y_act<-as.factor(y_act)
cm<-confusionMatrix(data=y_pred, 
  reference=y_act)

Accuracy<-round(cm$overall[1],2)
Accuracy
```
<br><br>

<br>
Prediction on the dataset, if p>0.5 then class as 1, otherwise 0 and check the accuracy. As we can see from the result, the prediction of the logistic model is 54% accuracy, which is okay. But, it doesn’t reveal much information about how well the model actually did in predicting the 1’s and 0’s independently.
<br>


#### The Confusion Matrix

```{r}
cm 
```
Sensitivity is the percentage of actual 1’s that were correctly predicted. It shows what percentage of 1’s were covered by the model. The sensitivity is 69.69%, which is okay.
Likewise, Specificity is the proportion of actual 0’s that were correctly predicted. So in this case, it is 42.77%, which does not perform well.
<br><br>


#### Lasso Regression for regularization

We use lasso regression to remove extra predictors and push them to 0,to try to improve our model.
```{r message=FALSE}
xfactors <- model.matrix(reordered ~ order_dow + order_hour_of_day + days_since_prior_order + product_id, data=OrderProductPrior_log)[, -1]
x <- as.matrix(data.frame(xfactors))

# Note alpha=1 for lasso only and can blend with ridge penalty down to
# alpha=0 ridge only.
glmmod <- glmnet(x, y=as.factor(OrderProductPrior_log$reordered), alpha=1, family="binomial")
```
<br><br>

##### Lasso plot and ROC


Each colored line in the lasso plot represents the value taken by a different coefficient in the model. Lambda is the weight given to the regularization term (the L1 norm), so as lambda approaches zero, the loss function of your model approaches the OLS loss function (David Marx,2013).

The second plot is the ROC. It shows the lambda values and their MSE. $\lambda$min and $\lambda$1se are both shown by the vertical line.
<br>

```{r}
# Plot variable coefficients vs. shrinkage parameter lambda.

plot(glmmod, xvar="norm", label=TRUE)

ROCcurve<-cv.glmnet(x,OrderProductPrior_log$reordered)
plot(ROCcurve)

sprintf("Minimum value of lambda that minimizes mean CV error: %# .7f", ROCcurve$lambda.min)


```

<br><br>

####  Get estimated beta matrix

Using the min lambda value found, we find the estimated beta matrix. This shows which coefficients have been shrunk to zero and which still exist. This shows which are the important variables that explain the variation in the dependent variable y. 

<br>

```{r}

newmodel_cv<-glmnet(x,OrderProductPrior_log$reordered,lambda=ROCcurve$lambda.min, standardize = TRUE)
newmodel_cv$beta

set.seed(123)
control_cv<-trainControl(method = "cv",number=10)
lassoGrid_train<-expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0005))
lasso_model_train<-train(x=x,y=as.factor(OrderProductPrior_log$reordered), method = 'glmnet', trControl = control_cv, tuneGrid = lassoGrid_train)
lasso_model_train$bestTune

max(lasso_model_train$results$Accuracy)


#lambda 1 standard error away
ROCcurve$lambda.1se

model_1se<- glmnet(x, OrderProductPrior_log$reordered, lambda = ROCcurve$lambda.1se)
model_1se$beta
```

<br><br>

More variables are removed upon using the lambda 1 standard deviation away. The 1 se rule is a standard one when performing lasso regression. The main point of the 1 SE rule is to choose the simplest model whose accuracy is comparable with the best model, according to (Friedman, Hastie, and Tibshirani,2010).

<br><br>


<center>
$\large Association~Rules~Mining$ </center>

<br><br>

For the Market Basket Analysis, only the order_products__prior and product are utilized and the same are joined on basis of product id. 
<br>
```{r}

basket_data <- left_join(OrderProductPrior, products, by='product_id')%>%
              group_by(order_id)%>%
              summarise(items=as.vector(list(product_name)))
head(basket_data)
```

<br>
Since the eclat and apriori functions are only valid on transaction format, henceforth the format is converted to the transaction type with the help of below code snippet. 

```{r}
transactions=as(basket_data$items, 'transactions')
head(transactions)
length(transactions)
```

#### Setting the Support and Confidence Intervals<br>

Understanding support<br>

A value of Support =0.02 means that an item will be considered as frequent if at least 2 percent of all the baskets contain it.

Understanding Confidence<br>

Confidence is a measure of the strength of an association rule. It is the frequency of occurrence of the right-hand items in the rule from among those baskets that contain the items on the left-hand side of the rule.

So now we have created a function that will plot the number of rules we can generate depending on different support levels and varied upon different confidence levels. 
<br>
```{r}
# Support and confidence values
supportLevels <- c(0.1, 0.05, 0.01, 0.005)
confidenceLevels <- c(0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)

# Empty integers 
rules_sup10 <- integer(length=9)
rules_sup5 <- integer(length=9)
rules_sup1 <- integer(length=9)
rules_sup0.5 <- integer(length=9)

# Apriori algorithm with a support level of 10%
for (i in 1:length(confidenceLevels)) {
  
  rules_sup10[i] <- length(apriori(transactions, parameter=list(sup=supportLevels[1], 
                                   conf=confidenceLevels[i], target="rules")))
  
}

# Apriori algorithm with a support level of 5%
for (i in 1:length(confidenceLevels)){
  
  rules_sup5[i] <- length(apriori(transactions, parameter=list(sup=supportLevels[2], 
                                  conf=confidenceLevels[i], target="rules")))
  
}

# Apriori algorithm with a support level of 1%
for (i in 1:length(confidenceLevels)){
  
  rules_sup1[i] <- length(apriori(transactions, parameter=list(sup=supportLevels[3], 
                                  conf=confidenceLevels[i], target="rules")))
  
}

# Apriori algorithm with a support level of 0.5%
for (i in 1:length(confidenceLevels)){
  
  rules_sup0.5[i] <- length(apriori(transactions, parameter=list(sup=supportLevels[4], 
                                    conf=confidenceLevels[i], target="rules")))
  
}
```




```{r}
# Number of rules found with a support level of 10%
plot1 <- qplot(confidenceLevels, rules_sup10, geom=c("point", "line"), 
               xlab="Confidence level", ylab="Number of rules found", 
               main="Apriori with a support level of 10%") +
  theme_bw()

# Number of rules found with a support level of 5%
plot2 <- qplot(confidenceLevels, rules_sup5, geom=c("point", "line"), 
               xlab="Confidence level", ylab="Number of rules found", 
               main="Apriori with a support level of 5%") + 
  scale_y_continuous(breaks=seq(0, 10, 2)) +
  theme_bw()

# Number of rules found with a support level of 1%
plot3 <- qplot(confidenceLevels, rules_sup1, geom=c("point", "line"), 
               xlab="Confidence level", ylab="Number of rules found", 
               main="Apriori with a support level of 1%") + 
  scale_y_continuous(breaks=seq(0, 50, 10)) +
  theme_bw()

# Number of rules found with a support level of 0.5%
plot4 <- qplot(confidenceLevels, rules_sup0.5, geom=c("point", "line"), 
               xlab="Confidence level", ylab="Number of rules found", 
               main="Apriori with a support level of 0.5%") + 
  scale_y_continuous(breaks=seq(0, 130, 20)) +
  theme_bw()

# Subplot
plot1 
plot2 
plot3
plot4
```


```{r}
rules1 <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5, maxlen=3), control = list(verbose = FALSE))
as(rules1,"data.frame")
rules2 <- apriori(transactions, parameter = list(supp = 0.001, conf = 0.4, maxlen=3), control = list(verbose = FALSE))
as(rules2,"data.frame")
rules3 <- apriori(transactions, parameter = list(supp = 0.005, conf = 0.1, maxlen=3), control = list(verbose = FALSE))
as(rules3,"data.frame")

plot(rules2, method="paracoord",  control=list(alpha=.5, reorder=TRUE))

plot(rules2, method = "grouped", control = list(k = 5))
```
<br><br>


#### Association Rules Mining using Gradient Boosting

```{r}
# xgboost- Gradient Boosting
library(xgboost)
# Creating a product level by combing product, department, aisles
product_level <- merge(x = products, y = aisles, by = "aisle_id")
product_level <- merge(x = product_level, y = departments, by = "department_id")
product_level$department_id <- NULL
product_level$aisle_id <- NULL
product_level <- arrange(product_level, product_id)

# removing unused data
rm(aisles,departments, products)
gc()

# !! Remember to take only order id in training set that are also in orders

# combining order data and prior data
ordered_products <- merge(x = orders, y = order_products_prior, by = "order_id")
rm(order_products_prior)
gc()



# product reorder probability and avg_cart_position of each cart
product_probab <- ordered_products %>% arrange(user_id, order_number, product_id) %>% group_by(product_id) %>% 
  summarise(product_orders = n(), product_reorders = sum(reordered), avg_cart_pos = mean(add_to_cart_order))
product_probab$reorder_probab <- product_probab$product_reorders/product_probab$product_orders
product_probab$product_reorders <- NULL

# calculating user buy probab

# calculating user order probability by looking at its prior orders 
users_probab <- orders %>% filter(eval_set == "prior") %>% group_by(user_id) %>% 
  summarise(user_orders = max(order_number), user_period = sum(days_since_prior_order, na.rm = TRUE),
    avg_days_since_prior = mean(days_since_prior_order, na.rm = TRUE))

# calculating total_products, reorder probability and num of products
users_reorder_probab <- ordered_products %>% group_by(user_id) %>%
  summarise(user_total_products = n(), user_reorder_probab = sum(reordered == 1) / sum(order_number > 1),
    num_products = n_distinct(product_id))

# merging above two to get user level
users_probab <- merge(x = users_probab, y = users_reorder_probab, by = "user_id", all.x = TRUE)

# filtering training and testing data from orders
train_test <- orders %>% filter(eval_set != "prior") %>% select(user_id, order_id, eval_set, days_since_prior_order)

# left join users_probab with train_test data
users_probab <- merge(x = users_probab, y = train_test, all.x = TRUE)

rm(train_test)


# calculating average cart position and total orders of each product user purchased
user_product_cart <- ordered_products %>% group_by(user_id, product_id) %>% 
  summarise( user_product_orders = n(), avg_user_product_pos = mean(add_to_cart_order))


rm(users_reorder_probab, product_level,ordered_products)
gc()


# now combining all the user_level and product _level info
user_product_cart <- merge(user_product_cart,product_probab, by = "product_id", all.x = TRUE)
user_product_cart <- merge(user_product_cart,users_probab, by = "user_id", all.x = TRUE)


#taking only user_ids that are common in orders
order_products_train$user_id <- orders$user_id[match(order_products_train$order_id, orders$order_id)]
# combining training data and infered data by product id and user id
order_products_train <- order_products_train %>% select(user_id, product_id, reordered)
user_product_cart <- merge(user_product_cart, order_products_train, by = c("user_id", "product_id"), all.x = TRUE)


rm(product_probab, order_products_train, users_probab, orders)
gc()

# training data
train <- as.data.frame(user_product_cart[user_product_cart$eval_set == "train",])
# removing char - xgboost
train$eval_set <- NULL
#no need
train$user_id <- NULL
train$product_id <- NULL
train$order_id <- NULL
train$reordered[is.na(train$reordered)] <- 0

# testing data
test <- as.data.frame(user_product_cart[user_product_cart$eval_set == "test",])
test$eval_set <- NULL
test$user_id <- NULL
test$reordered <- NULL

rm(user_product_cart)
gc()

#we got our training and testing data
# we have taken only numeric data because we will be using xgboost




params <- list(
  # logistic model
  "objective"           = "reg:logistic",
  
  # logless for cross-validation
  "eval_metric"         = "logloss", 
  
  #learning rate
  "eta"                 = 0.1,
  
  #depth of tree
  "max_depth"           = 6, 
  
  # min sum of weights
  # should be high enough to prevent over fitting
  # but not too high for over fitting
  "min_child_weight"    = 10,
  
  # the min loss value require to split
  "gamma"               = 0.70,
  
  # fraction of observations to be included in each tree 
  # generally varies from 0.5-1
  "subsample"           = 0.75,
  
  # fraction of column to be randomly sample in each tree
  "colsample_bytree"    = 0.95,
  
  # regularization coefficients
  "alpha"               = 2e-05,
  "lambda"              = 10 
)

# taking 1% of data
subtrain <- train %>% sample_frac(0.1)
X <- xgb.DMatrix(as.matrix(subtrain %>% select(-reordered)), label = subtrain$reordered)
model <- xgboost(data = X, params = params, nrounds = 80)

rm(X, subtrain)
gc()

# predicting reordered values from test dataset
X <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))
test$reordered <- predict(model, X)

test$reordered <- (test$reordered > 0.21) * 1

# summarise as order_id and products
rules <- test %>% filter(reordered == 1) %>% group_by(order_id) %>%
 rules(products = paste(product_id, collapse = " "))

# filliing the missing values
missing <- data.frame(
  order_id = unique(test$order_id[!test$order_id %in% rules$order_id]), products = "None")
rules <- rules %>% bind_rows(missing) %>% arrange(order_id)

head(rules,20)


```




