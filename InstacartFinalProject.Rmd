---
title: "InstacartFinalProject"
author: "Shivangi Vashi"
date: "6/23/2020"
output:
  html_document: default
  pdf_document: default
number_sections: yes
---
<center>


### ALY 6040 Data Mining Applications
### Instacart Final Project
### Shivangi Vashi
### Yihong Qiu
### Md Tajrianul Islam

<br> <br> <br> <br>
<br> <br> <br> <br>
  
### Instructor: Kasun Samarasinghe
### Spring 2020
### June 23 2020
### Northeastern University
</center>

<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>
<br> <br> <br> <br>

<style>
body {
text-align: justify}
</style>

<center>
$\LARGE Introduction$  </center>
<br><br>

Instacart is a grocery delivery application that works in the following way- users can select products through the app, and then personal shoppers review the order, shop in-store and deliver the products to the users.
We got the data from Instacart's Market Basket Analysis on kaggle.com [here](https://www.kaggle.com/c/instacart-market-basket-analysis/overview).

The Instacart data set is anonymized and contains samples of over 3 million grocery orders from 200,000+ Instacart users.  <br>

The reason we chose this data is because it is varied and some very clear applications for resolving business questions. It will allow us to perform different data mining algorithms and analyze it in different ways. It also has time information with enough granularity, so the option of some time series analysis is also there. <br><br>

In this final project, there are four parts of the Instacart Market Basket analysis. The first part is EDA. We explore the frequency of reordering, when in a day or week do users order the most, popular products, and so on. This gives us an idea about user buying patterns. The other parts is using different methods to solve the business problem.

Business Problem:

On its website Instacart has a recommendation system, that suggests the users some items to buy again. Our task is to predict which items will be reordered on the next order.

We approach this using 3 methods:
-Logistic Regression: We use logistic regression to predict if a product will be reordered or not. Since the 'reordered' variable is binary, this is a classification problem that can be solved using logistic regression

-Association Rule Mining:
We use the apriori algorithm perform market basket analysis, to generate association rules with different values of support and confidence, to predict what products are closely associated with or frequently bought with what products 

-Gradient Boosting 
<br><br>


Reading the dataset,importing relevant libraries.

```{r global_options, include=FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r message=FALSE,warning=FALSE}
library(plyr)
library(data.table)
library(tidyverse)
library(arules)
library(arulesViz)
library(plotly)
library(IRdisplay)
library(grid)
library(caret)
library(glmnet)
library(e1071)


#using fread because it reads data very fast
aisles<-fread("instacart-market-basket-analysis/aisles.csv")
departments<-fread("instacart-market-basket-analysis/departments.csv")
order_products_prior<-fread("instacart-market-basket-analysis/order_products__prior.csv")
order_products_train<-fread("instacart-market-basket-analysis/order_products__train.csv")
orders<-fread("instacart-market-basket-analysis/orders.csv")
products<-fread("instacart-market-basket-analysis/products.csv")

```
<br><br>


<center>
$\large Data~Wrangling$ </center>
<br><br>

#### Data Preparation

<br>
Since the dataset is very large, with Prior orders having 32 million rows, we subset the data to reduce calculation time. We did this by randomly sampling users, then only keeping their orders and prior order information by performing inner joins with the order prior and train datasets.
<br>

```{r}
set.seed(123)
user_fraction <- 0.1
users <- unique(orders$user_id)
sample_users <- sample(users, round(user_fraction * length(users)))

cat('Number of orders (before): ', nrow(orders))
orders <- orders[user_id %in% sample_users]
cat('Number of orders (after): ', nrow(orders))

# Training dataset
OrderProductPrior<-orders%>%inner_join(order_products_prior)
OrderProductPrior<-drop_na(OrderProductPrior)


#Testing dataset
OrderProductTrain<-orders%>%inner_join(order_products_train)
OrderProductTrain<-drop_na(OrderProductTrain)


order_products_prior<-order_products_prior%>%semi_join(orders)
order_products_train<-order_products_train%>%semi_join(orders)
dim(order_products_prior)
dim(order_products_train)

dim(OrderProductPrior)
dim(OrderProductTrain)
head(OrderProductPrior)
head(OrderProductTrain)

```
<br><br>


```{r message=FALSE}
# Taking a quick look at the data
head(aisles)
head(departments)
head(order_products_prior)
head(order_products_train)
head(orders)
head(products)

#Remove missing values
aisles<-drop_na(aisles)
departments<-drop_na(departments)
order_products_prior<-drop_na(order_products_prior)
order_products_train<-drop_na(order_products_train)
orders<-drop_na(orders)
products<-drop_na(products)


# Check if all classes ie data types is correct
sapply(aisles,class)
sapply(departments,class)
sapply(order_products_prior,class)
sapply(order_products_train,class)
sapply(orders,class)
orders<-orders %>%mutate(order_hour_of_day=as.numeric(order_hour_of_day))
head(orders)
sapply(products,class)

#We recode and convert character variables to factors.
orders$eval_set <-as.factor(orders$eval_set)
aisles$aisle <- as.factor(aisles$aisle)
departments$department <- as.factor(departments$department)
products$product_name <- as.factor(products$product_name)
```
<br><br>

<center>
${\large Data Exploration}$ </center>
<br><br>

#### 1. Ordering patterns 
<br>
First, we find out most orders are placed during 9am to 5 pm during the day. Additionally, assuming Sunday is the first, ie 0 = Sunday in the dataset, Sundays and Mondays are when most orders are placed.
<br>

```{r warning=FALSE}
# When do people order most during the day?
orders%>%
  group_by(order_hour_of_day)%>%
  summarise(Number_of_Orders=n())%>%
  ggplot(aes(y=Number_of_Orders, x=order_hour_of_day, fill=Number_of_Orders)) + geom_col()+ coord_cartesian(xlim = c(0, 24))+ labs(y="Number of Orders", x="Hour of the Day")

#What days of the week do people order during the week
orders%>%
  group_by(order_dow)%>%
  summarise(Number_of_Orders=n())%>%
  ggplot(aes(y=Number_of_Orders, x=order_dow, fill=Number_of_Orders)) + geom_col()+ labs(y="Number of Orders", x="Day of the Week starting Sunday" )


```
<br><br>

Using this information, Instacart could redirect their resources so that the higher volume of orders can be processed at these times efficiently. <br>
<br> <br>

#### 2. What is the frequency of reordering?

<br>
When exploring the frequency of reordering, first we filter prior eval_set and select data of days_since_prior_order, then we also clease all the na value from the dataset. 
We find out in this 3,000,000 orders+ dataset, the average days of users who reorder their groceries is from 7 to 11 days. Many customers reorder after 7 days or 30 days.
According to each customer's reordering behavior and how many days they would reorder through instacart,we can predict their next ordering day. Therefore, based on these analysis, it can provide suggestions to retails and suppliers which would be helpful for their purchase sales inventory stategies.
<br>
```{r}
summary(orders$days_since_prior_order)

ggplot(orders, aes(x=days_since_prior_order))+ 
  geom_boxplot(mapping=aes("var",days_since_prior_order),colour="black",fill="#AC88FF")+
  xlab("")+
  ylab("days_since_prior_order")+
  ggtitle("Days since Prior Order Boxplot")

ggplot(orders, aes(x=days_since_prior_order))+ 
  geom_histogram(aes(y =..density..),stat = "bin", bins= 30, colour="black", fill="lavenderblush")+
  geom_density(alpha=.2,fill="plum")+
  ggtitle("Days since Prior Order Density Plot")

```
<br><br>


#### 3. What are the most popular products?
<br>
By descending ordering products that are added into cart order, we find out the top 10 popular products out of 49,688 products as shown below. Surprisely, in our analysis, the top No.1 popular product is Organic Ezekiel 49 Bread Cinnamon Raisin.

It is important to find out what products people like to order and order the most, by doing so, we can have better prediction and preparation on the stock of these products. 
In our next steps, we will find out which departmants and aisles these products belong to, and how often do these top products reorder?
<br><br>
```{r}
inner_join(order_products_prior, products, by =c("product_id"))%>%
        as.data.frame()%>%
        select(5, 3)%>%
        head(arrange(desc(pop_products$add_to_cart_order)), n = 10)%>%
        ggplot()+
        geom_bar(aes(reorder(product_name,-add_to_cart_order),add_to_cart_order, fill= add_to_cart_order),
        stat = "identity", color = "grey", fill = "#00BFC4") +
        labs(title = "Most Popular Products Added to Cart Order", x = "product names", y = "add to cart order") +
        theme(axis.text.x = element_text(angle = 70, hjust = 1))

```
<br><br>


### 4. Which Users Reorder?
<br>
To recommend the next item an user is most likely to add to his/ her cart, we have to understand how each group of users use the service. It becomes a classification problem that we can explore later, but for now we wanted to see how different users purchase products, how there can be correlation between their total number of orders, days since prior order, no. of items they add to their cart and their probability of reordering.   
<br>
```{r}
#user clusters
n_items_per_order <- OrderProductPrior %>% group_by(order_id) %>% mutate(n_items=max(add_to_cart_order))
user <- n_items_per_order %>% 
  group_by(user_id) %>% 
  mutate(product_name = as.numeric(days_since_prior_order)) %>%
  summarize(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = mean(n_items), avg_reorder=mean(reordered))
head(user)
```
<br><br>

<br>
So what we can see from the table the people who have ordered more number of times, they have higher possibility of reordering the same products and they usually order after every three to four weeks. Which means that the products that gets ordered more will also have the higher probablity of being reordered.
<br>

```{r}
order_products_train %>% 
  group_by(product_id) %>% 
  summarize(proportion_reordered = mean(reordered), n=n()) %>%
  ggplot(aes(x=n,y=proportion_reordered))+
  geom_point()+
  geom_smooth(color="red")+
  coord_cartesian(xlim=c(0,2000))

```
<br><br>


#### 5. Most popular department and most popular aisle?

<br>
So we wanted to see which are the most popular departments and aisles, as aparently these are the same department and aisles where will most reorder must occur. Later we can also find if which other product has the more probablity of being purchased with other. We can find that using association rule mining. To see the most popular departments and aisles we create a treemap where the size of the boxes shows the number of sales.
<br>
```{r message=FALSE}
library(treemap)
tmp <- products %>%
        group_by(department_id, aisle_id) %>% summarize(n=n())%>%
        left_join(departments,by="department_id")%>%
        left_join(aisles,by="aisle_id")

order_products_train %>% 
  group_by(product_id) %>% 
  summarize(count=n()) %>% 
  left_join(products,by="product_id") %>% 
  ungroup() %>% 
  group_by(department_id,aisle_id) %>% 
  summarize(sumcount = sum(count)) %>% 
  left_join(tmp, by = c("department_id", "aisle_id")) %>% 
  mutate(onesize = 1)%>%
  treemap(product_portfolio,index=c("department","aisle"),vSize="sumcount",title="",palette="Set3",border.col="#FFFFFF")
```

<br><br>
What we can see is produce, produce and diary eggs are the most popular departments followed by snacks, pantry and others. But talking about departments, organic and non organic sector divides US consumers in a large way these days. So it will be interesting to see which products get most reordered.

<br>


<center>
$\large Logistic~Regression$ </center>
<br><br>

```{r}
## Preparing data for modelling
OrderProductPrior_log<-OrderProductPrior%>%inner_join(products)
OrderProductPrior_log<-drop_na(OrderProductPrior_log)
OrderProductPrior_log<-OrderProductPrior_log[-c(3)]

OrderHour <- OrderProductPrior_log[-c(1,2,7,10)]

OrderProductTrain_log<-orders%>%inner_join(order_products_train)%>%inner_join(products)
OrderProductTrain_log<-drop_na(OrderProductTrain_log)
OrderProductTrain_log<-OrderProductTrain_log[-c(3)]
```
<br><br>

<br>
We perform logistic regression to predict the variable 'reorder'. Since it is a binary variable with 2 classes,1 and 0, it becomes a classification problem.
As we know from our EDA, Instacart has a lot of loyal customers, who order bi-weekly or monthly. While ordering they also reorder a lot of the same products. So predicting whether a user will reorder or not, may help us later to understand what products to recommend while they are purchasing.
<br>

```{r}
user <- OrderProductPrior_log %>% 
  group_by(user_id) %>% 
  mutate(days_since_prior_order = as.numeric(days_since_prior_order)) %>%
  transmute(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = max(add_to_cart_order), reordered= reordered, order_day = order_dow, product_id=product_id)

reorder_log <- user[-c(1)]
head(reorder_log)
```
<br><br>

<br>
So we created a new dataframe dropped the user_id and use all the other columns to predict the reorders with logistic regression. Our regression depends variables such as the total number orders a user has, after how many days he is ordering, how many products he is adding to the cart, which day he is ordering, and which products usually get reordered. For a example if a user usually shops bi weekly, on sunday and orders milk; he might have very less chances of ordering milk if he is back within 2 or 3 days of his last purchase.
<br>

#### Building Model
We use glm with family="binomial" to create the model. We use all of the variables for this model.
```{r}
#Fitting a binary logistic regression
log_model <- glm(reordered ~., data = reorder_log, family = "binomial")
#Model summary
summary(log_model)

```


#### Predicting the model

We predict the reorder variable using predict function.
```{r}
user2 <- OrderProductTrain_log %>% 
  group_by(user_id) %>% 
  mutate(days_since_prior_order = as.numeric(days_since_prior_order)) %>%
  transmute(total_orders = n_distinct(order_id), avg_days_since_prior_order= mean(days_since_prior_order, na.rm = TRUE), avg_no_items = max(add_to_cart_order), reordered= reordered, order_day = order_dow, product_id=product_id)

reorder_log2 <- user2[-c(1)]
head(reorder_log2)

#Prediction
pred <- predict(log_model, reorder_log2, type = "response")
#If p > 0.5, then Class is 1 else 0
y_pred <- ifelse(pred > 0.5, 1, 0)
y_act <- reorder_log2$reordered

#Accuracy
mean(y_pred == y_act) 

## Another method
y_pred<-as.factor(y_pred)
y_act<-as.factor(y_act)
cm<-confusionMatrix(data=y_pred, 
  reference=y_act)

Accuracy<-round(cm$overall[1],2)
Accuracy
```
<br><br>

<br>
Prediction on the dataset, if p>0.5 then class as 1, otherwise 0 and check the accuracy. As we can see from the result, the prediction of the logistic model is 54% accuracy, which is okay. But it doesn’t reveal much information about how well the model actually did in predicting the 1’s and 0’s independently.
<br>


#### The Confusion Matrix

```{r}
cm 
```
Sensitivity is the percentage of actual 1’s that were correctly predicted. It shows what percentage of 1’s was covered by the model. The sensitivity is 69.69%, which is okay.
Likewise, Specificity is the proportion of actual 0’s that were correctly predicted. In this case, it is 42.77%, which does not perform well.
<br><br>


#### Lasso Regression for regularization

We use lasso regression to remove extra predictors and push them to 0,to try to improve our model.
```{r message=FALSE}
xfactors <- model.matrix(reordered ~ order_dow + order_hour_of_day + days_since_prior_order + product_id, data=OrderProductPrior_log)[, -1]
x <- as.matrix(data.frame(xfactors))

# Note alpha=1 for lasso only and can blend with ridge penalty down to
# alpha=0 ridge only.
glmmod <- glmnet(x, y=as.factor(OrderProductPrior_log$reordered), alpha=1, family="binomial")
```
<br><br>

##### Lasso plot and ROC
<br>
Each colored line in the lasso plot represents the value taken by a different coefficient in the model. Lambda is the weight given to the regularization term (the L1 norm), so as lambda approaches zero, the loss function of your model approaches the OLS loss function (David Marx,2013). <br>

The second plot is the ROC. It shows the lambda values and their MSE. $\lambda$min and $\lambda$1se are both shown by the vertical line.
<br><br>

```{r}
# Plot variable coefficients vs. shrinkage parameter lambda.

plot(glmmod, xvar="norm", label=TRUE)

ROCcurve<-cv.glmnet(x,OrderProductPrior_log$reordered)
plot(ROCcurve)

sprintf("Minimum value of lambda that minimizes mean CV error: %# .7f", ROCcurve$lambda.min)


```
<br><br>

####  Get estimated beta matrix

<br>
Using the min lambda value found, we find the estimated beta matrix. This shows which coefficients have been shrunk to zero and which still exist. This shows which are the important variables that explain the variation in the dependent variable y. 
<br>

```{r}

newmodel_cv<-glmnet(x,OrderProductPrior_log$reordered,lambda=ROCcurve$lambda.min, standardize = TRUE)
newmodel_cv$beta

set.seed(123)
control_cv<-trainControl(method = "cv",number=10)
lassoGrid_train<-expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0005))
lasso_model_train<-train(x=x,y=as.factor(OrderProductPrior_log$reordered), method = 'glmnet', trControl = control_cv, tuneGrid = lassoGrid_train)
lasso_model_train$bestTune

max(lasso_model_train$results$Accuracy)


#lambda 1 standard error away
ROCcurve$lambda.1se

model_1se<- glmnet(x, OrderProductPrior_log$reordered, lambda = ROCcurve$lambda.1se)
model_1se$beta
```

<br><br>
More variables are removed upon using the lambda 1 standard deviation away. The 1 se rule is a standard one when performing lasso regression. The main point of the 1 SE rule is to choose the simplest model whose accuracy is comparable with the best model, according to (Friedman, Hastie, and Tibshirani,2010).
<br><br>




